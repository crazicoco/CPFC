Parameters:
root    :    /share/home/crazicoco/competition/CPFC
vocabIdName    :    vocab.txt
tokenizeModel    :    hfl/chinese-roberta-wwm-ext-large
pretrainModel    :    hfl/chinese-roberta-wwm-ext-large
saveModelAddress    :    saveModelBin/
processedDataDir    :    preprocessed_data
saveLabelIdName    :    label.pt
saveTrainIdName    :    train.pt
saveValidIdName    :    valid.pt
saveTestIdName    :    test.pt
batch_size    :    9
epoch_size    :    20
loss_calculate    :    cross-entroy
lr    :    2e-05
device    :    0
max_len    :    512
lossCalculateWay    :    general
accumulate    :    True
ifparallel    :    True
debug    :    False
logfileName    :    public
a_step    :    16
description    :    change the weight for every label and add the fgm
use_bert_layer    :    -1
record_addr    :    /share/home/crazicoco/competition/CPFC/record_analysis/2021_1_3_18_14
*********************************************train model**************************************
current epoch:0
[ 1000 - th batch: train loss is 5.240159 valid total precise is 0.381381 ]
[ 2000 - th batch: train loss is 3.831219 valid total precise is 0.449169 ]
[ 3000 - th batch: train loss is 3.177443 valid total precise is 0.486051 ]
[ 4000 - th batch: train loss is 2.775364 valid total precise is 0.510322 ]
[ 5000 - th batch: train loss is 2.503380 valid total precise is 0.529195 ]
[ 6000 - th batch: train loss is 2.300400 valid total precise is 0.544091 ]
[ 7000 - th batch: train loss is 2.147978 valid total precise is 0.555571 ]
[ 8000 - th batch: train loss is 2.025952 valid total precise is 0.564154 ]
[ 9000 - th batch: train loss is 1.929369 valid total precise is 0.569594 ]
[ 10000 - th batch: train loss is 1.847569 valid total precise is 0.575502 ]
[ 11000 - th batch: train loss is 1.775974 valid total precise is 0.581608 ]
[ 12000 - th batch: train loss is 1.716786 valid total precise is 0.586438 ]
[ 13000 - th batch: train loss is 1.666676 valid total precise is 0.590396 ]
[ 14000 - th batch: train loss is 1.622435 valid total precise is 0.593701 ]
[ 15000 - th batch: train loss is 1.582621 valid total precise is 0.596536 ]
The epoch:0,marcoF1ScoresOcemotion:0.453989
********************************confusion_matrix********************************
[[1166  356    9  397   80 1756   19]
 [ 441 1346   13  514  115 1586   16]
 [  24   23  150   45   36  267    3]
 [ 206  253   10 5724  591 1393   23]
 [  79   77    7 1003 1562  992   14]
 [ 595  449   31  945  447 9075   26]
 [  99   72    1  204   51  301  102]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.45      0.31      0.36      3783
           1       0.52      0.33      0.41      4031
           2       0.68      0.27      0.39       548
           3       0.65      0.70      0.67      8200
           4       0.54      0.42      0.47      3734
           5       0.59      0.78      0.67     11568
           6       0.50      0.12      0.20       830

    accuracy                           0.58     32694
   macro avg       0.56      0.42      0.45     32694
weighted avg       0.57      0.58      0.56     32694

The epoch:0,marcoF1ScoresOcnli:0.667766
********************************confusion_matrix********************************
[[11051  4194  1470]
 [ 3602 11148  2534]
 [ 1626  3419 11343]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.68      0.66      0.67     16715
           1       0.59      0.64      0.62     17284
           2       0.74      0.69      0.71     16388

    accuracy                           0.67     50387
   macro avg       0.67      0.67      0.67     50387
weighted avg       0.67      0.67      0.67     50387

The epoch:0,marcoF1ScoresTnews:0.532244
********************************confusion_matrix********************************
[[ 522  125  263    6   16   17   27   48   31   21   25   74    0   80
    11]
 [  53 2638  449   45   82   18   28  277  117   98  424  148    0  172
    43]
 [ 170  437 3474  222   58   27   52  163  217  108  111  225    0   79
   263]
 [   7  116  474 3000   62    8   81  106   87   70   82  158    0   26
   260]
 [  13   92   84   50 2772  256  143  176 1332   51  143  354   44  336
    35]
 [  22   48   39   16  321 1363   44   70  102    8  172   51    1  104
     5]
 [  30   72  158   55  174   39 2966   81  464   87  252  154    0   80
    63]
 [  27  324  189   78  122   50   49 2300  258   67  116  154    0   97
    52]
 [   7   86  143   53 1245   65  397  228 3657  153   74  208    6  105
   261]
 [  13  138  138   63   61    5   92   88  178 1935   77 1161    0   44
   135]
 [  23  441  134   37  117   96  168   96  143   72 1815  409    0  289
    36]
 [  90  136  298   79  249   41  102  136  234  813  300 2838    1  112
    41]
 [   0    1    0    2  211    3    2    0   14    0    1    6   48    1
     1]
 [  82  179   88   13  319   87   61  108  134   35  307  132    0 1671
    15]
 [  14  101  417  215   43    5   55   65  385  145   36   59    0   33
  2298]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.49      0.41      0.45      1266
           1       0.53      0.57      0.55      4592
           2       0.55      0.62      0.58      5606
           3       0.76      0.66      0.71      4537
           4       0.47      0.47      0.47      5881
           5       0.66      0.58      0.61      2366
           6       0.70      0.63      0.66      4675
           7       0.58      0.59      0.59      3883
           8       0.50      0.55      0.52      6688
           9       0.53      0.47      0.50      4128
          10       0.46      0.47      0.46      3876
          11       0.46      0.52      0.49      5470
          12       0.48      0.17      0.25       290
          13       0.52      0.52      0.52      3231
          14       0.65      0.59      0.62      3871

    accuracy                           0.55     60360
   macro avg       0.56      0.52      0.53     60360
weighted avg       0.56      0.55      0.55     60360

The epoch:0,marcoF1ScoresTotal:0.551333
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.853130 valid total precise is 0.656879 ]
The epoch:0,marcoF1ScoresOcemotion:0.491616
********************************confusion_matrix********************************
[[116  23   0  45   6 137   5]
 [ 54 127   0  65   5 119   4]
 [  1   2  15  10   1  17   0]
 [ 17  15   1 635  24  89   0]
 [  6   8   3 142 122  69   3]
 [ 58  39   3 124  28 782   2]
 [  8   3   0  26   0  22  19]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.45      0.35      0.39       332
           1       0.59      0.34      0.43       374
           2       0.68      0.33      0.44        46
           3       0.61      0.81      0.69       781
           4       0.66      0.35      0.45       353
           5       0.63      0.75      0.69      1036
           6       0.58      0.24      0.34        78

    accuracy                           0.61      3000
   macro avg       0.60      0.45      0.49      3000
weighted avg       0.60      0.61      0.58      3000

The epoch:0 ,marcoF1ScoresOcnli:0.801929
********************************confusion_matrix********************************
[[850 115  46]
 [111 781 109]
 [ 58 156 774]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.83      0.84      0.84      1011
           1       0.74      0.78      0.76      1001
           2       0.83      0.78      0.81       988

    accuracy                           0.80      3000
   macro avg       0.80      0.80      0.80      3000
weighted avg       0.80      0.80      0.80      3000

The epoch:0,marcoF1ScoresTnews:0.524775
********************************confusion_matrix********************************
[[ 27   7   6   0   0   0   0   8   0   1   0   4   0   5   2]
 [  9 131  18   1   3   2   1  13   3   8  17   9   0   8   2]
 [ 12  22 151  10   1   2   3  12  14   8   6  17   0   3  19]
 [  1   6  16 158   2   1   4   7   4   2   2   5   0   0  13]
 [  3   4   1   2 121  13   3   7  83   2   3  12   0  18   3]
 [  1   3   0   1  19  71   1   5   6   1   7   1   0   3   0]
 [  4   3   2   4   7   4 134   4  39   2  18   7   0   1   5]
 [  4  20   2   7   7   3   1 125  12   4   2   6   0   6   1]
 [  0   3   5   5  50   3   9  17 217   6   4  13   0   5  19]
 [  2   6   2   4   5   0   5   6   7 134   1  36   0   2  10]
 [  3  20   7   4   1   7   1   7   7   4  83  21   0  19   1]
 [  6  12   9   5  12   2   3   7   5  87   8 126   0   3   1]
 [  0   0   0   0  11   0   0   0   1   0   0   0   0   0   0]
 [  2  10   3   2  17   7   2   2   6   0   7   2   0  88   1]
 [  1   2  12   4   0   0   0   3  21   2   3   5   0   1 124]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.36      0.45      0.40        60
           1       0.53      0.58      0.55       225
           2       0.65      0.54      0.59       280
           3       0.76      0.71      0.74       221
           4       0.47      0.44      0.46       275
           5       0.62      0.60      0.61       119
           6       0.80      0.57      0.67       234
           7       0.56      0.62      0.59       200
           8       0.51      0.61      0.56       356
           9       0.51      0.61      0.56       220
          10       0.52      0.45      0.48       185
          11       0.48      0.44      0.46       286
          12       0.00      0.00      0.00        12
          13       0.54      0.59      0.57       149
          14       0.62      0.70      0.65       178

    accuracy                           0.56      3000
   macro avg       0.53      0.53      0.52      3000
weighted avg       0.57      0.56      0.56      3000

The epoch:0,marcoF1ScoresTotal:0.606107
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2021_1_3_18_14/save_model/roberta_best_dev_f1_0.606106853033627.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2021_1_3_18_14/save_model/classifier_best_dev_f1_0.606106853033627.pt
best epoch is:0 with best f1 is: 0.606107
*********************************************train model**************************************
current epoch:1
[ 1000 - th batch: train loss is 0.590171 valid total precise is 0.696141 ]
[ 2000 - th batch: train loss is 0.576086 valid total precise is 0.699294 ]
[ 3000 - th batch: train loss is 0.564366 valid total precise is 0.698492 ]
[ 4000 - th batch: train loss is 0.559295 valid total precise is 0.699675 ]
[ 5000 - th batch: train loss is 0.554593 valid total precise is 0.701029 ]
[ 6000 - th batch: train loss is 0.551239 valid total precise is 0.701209 ]
[ 7000 - th batch: train loss is 0.548442 valid total precise is 0.700941 ]
[ 8000 - th batch: train loss is 0.544781 valid total precise is 0.701893 ]
[ 9000 - th batch: train loss is 0.542598 valid total precise is 0.702411 ]
[ 10000 - th batch: train loss is 0.540355 valid total precise is 0.702659 ]
[ 11000 - th batch: train loss is 0.539439 valid total precise is 0.703357 ]
[ 12000 - th batch: train loss is 0.536503 valid total precise is 0.705290 ]
[ 13000 - th batch: train loss is 0.535168 valid total precise is 0.706080 ]
[ 14000 - th batch: train loss is 0.533548 valid total precise is 0.706606 ]
[ 15000 - th batch: train loss is 0.533171 valid total precise is 0.706788 ]
The epoch:1,marcoF1ScoresOcemotion:0.586559
********************************confusion_matrix********************************
[[1734  304   11  268   52 1373   41]
 [ 473 1857    7  390   60 1207   37]
 [  21   24  230   32   25  211    5]
 [ 145  206   10 6490  466  852   31]
 [  56   51    4  819 2083  706   15]
 [ 589  412   29  598  314 9595   31]
 [ 100   69    2  141   37  237  244]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.56      0.46      0.50      3783
           1       0.64      0.46      0.53      4031
           2       0.78      0.42      0.55       548
           3       0.74      0.79      0.77      8200
           4       0.69      0.56      0.62      3734
           5       0.68      0.83      0.75     11568
           6       0.60      0.29      0.40       830

    accuracy                           0.68     32694
   macro avg       0.67      0.54      0.59     32694
weighted avg       0.68      0.68      0.67     32694

The epoch:1,marcoF1ScoresOcnli:0.818096
********************************confusion_matrix********************************
[[13802  2399   514]
 [ 2137 13628  1519]
 [  736  1924 13728]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.83      0.83      0.83     16715
           1       0.76      0.79      0.77     17284
           2       0.87      0.84      0.85     16388

    accuracy                           0.82     50387
   macro avg       0.82      0.82      0.82     50387
weighted avg       0.82      0.82      0.82     50387

The epoch:1,marcoF1ScoresTnews:0.614449
********************************confusion_matrix********************************
[[ 748   98  165    3   16   11   12   38    5   11   20   56    0   71
    12]
 [  75 3057  318   37   57   19   14  240   69   75  362  107    0  122
    40]
 [ 206  370 3753  198   44   14   48  153  123   86   93  216    0   64
   238]
 [  12   90  363 3338   48   10   61   92   44   66   69  106    0   18
   220]
 [  12   77   56   41 3461  221  107  144 1065   42   96  248   33  253
    25]
 [  24   37   21   11  303 1560   33   51   59    2  142   49    1   68
     5]
 [  31   69  117   37  173   39 3198   78  364   82  269  116    0   52
    50]
 [  39  297  127   80  100   61   24 2618  153   56   89  101    0   92
    46]
 [   3   62  113   27 1249   70  314  233 3991  136   45  153    3   74
   215]
 [  16  117   93   38   52    4   66   82  102 2359   90  967    0   25
   117]
 [  20  448   87   28   93  104  141   70   80   33 2095  361    0  274
    42]
 [  75  101  233   87  259   19   78  110  124  782  268 3240    0   57
    37]
 [   0    0    0    0  203    3    1    0   13    0    0    0   70    0
     0]
 [  93  148   47   10  308  106   38   95   58   31  233   91    0 1956
    17]
 [  13   73  358  124   38    3   38   61  328  121   33   33    0   30
  2618]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.55      0.59      0.57      1266
           1       0.61      0.67      0.63      4592
           2       0.64      0.67      0.66      5606
           3       0.82      0.74      0.78      4537
           4       0.54      0.59      0.56      5881
           5       0.70      0.66      0.68      2366
           6       0.77      0.68      0.72      4675
           7       0.64      0.67      0.66      3883
           8       0.61      0.60      0.60      6688
           9       0.61      0.57      0.59      4128
          10       0.54      0.54      0.54      3876
          11       0.55      0.59      0.57      5470
          12       0.65      0.24      0.35       290
          13       0.62      0.61      0.61      3231
          14       0.71      0.68      0.69      3871

    accuracy                           0.63     60360
   macro avg       0.64      0.61      0.61     60360
weighted avg       0.64      0.63      0.63     60360

The epoch:1,marcoF1ScoresTotal:0.673034
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.756450 valid total precise is 0.669225 ]
The epoch:1,marcoF1ScoresOcemotion:0.522812
********************************confusion_matrix********************************
[[116  30   1  33   8 133  11]
 [ 47 153   0  48  11 108   7]
 [  1   3  16   9   2  15   0]
 [ 17  17   1 596  56  94   0]
 [  7  11   3  78 183  69   2]
 [ 54  45   5  76  62 791   3]
 [  7   5   0  20   1  22  23]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.47      0.35      0.40       332
           1       0.58      0.41      0.48       374
           2       0.62      0.35      0.44        46
           3       0.69      0.76      0.73       781
           4       0.57      0.52      0.54       353
           5       0.64      0.76      0.70      1036
           6       0.50      0.29      0.37        78

    accuracy                           0.63      3000
   macro avg       0.58      0.49      0.52      3000
weighted avg       0.62      0.63      0.61      3000

The epoch:1 ,marcoF1ScoresOcnli:0.813555
********************************confusion_matrix********************************
[[869  89  53]
 [141 729 131]
 [ 44  98 846]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.86      0.84      1011
           1       0.80      0.73      0.76      1001
           2       0.82      0.86      0.84       988

    accuracy                           0.81      3000
   macro avg       0.81      0.81      0.81      3000
weighted avg       0.81      0.81      0.81      3000

The epoch:1,marcoF1ScoresTnews:0.557342
********************************confusion_matrix********************************
[[ 21   8  10   0   0   1   1   7   0   1   2   2   0   5   2]
 [  6 127  27   0   5   2   1  11   2   7  21   8   0   6   2]
 [  8  20 176   7   1   2   3  10  13   6   7  16   0   2   9]
 [  0   5  18 158   4   1   5   7   3   2   1   6   0   0  11]
 [  2   6   1   1 131  21   6   5  59   3   4  14   3  17   2]
 [  0   1   0   2  14  76   2   3   5   1  11   2   0   2   0]
 [  4   2   2   2   6   5 156   3  25   2  16   7   0   0   4]
 [  1  17   6   5   7   1   3 115  19   5   4   6   0   8   3]
 [  0   1   9   5  67   4  22  13 191   8   4  10   0   5  17]
 [  1   4   5   4   4   0   5   4  11 101   2  66   0   3  10]
 [  1  18  10   3   2   7   7   6   5   3  92  16   0  15   0]
 [  5  13  11   3  10   4   3   7   7  51  10 157   0   5   0]
 [  0   0   0   0   7   0   0   0   0   0   0   0   5   0   0]
 [  2   8   4   2  19   8   4   3   6   0   9   2   0  81   1]
 [  1   1  21   4   0   1   1   3  22   2   5   3   0   1 113]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.40      0.35      0.38        60
           1       0.55      0.56      0.56       225
           2       0.59      0.63      0.61       280
           3       0.81      0.71      0.76       221
           4       0.47      0.48      0.47       275
           5       0.57      0.64      0.60       119
           6       0.71      0.67      0.69       234
           7       0.58      0.57      0.58       200
           8       0.52      0.54      0.53       356
           9       0.53      0.46      0.49       220
          10       0.49      0.50      0.49       185
          11       0.50      0.55      0.52       286
          12       0.62      0.42      0.50        12
          13       0.54      0.54      0.54       149
          14       0.65      0.63      0.64       178

    accuracy                           0.57      3000
   macro avg       0.57      0.55      0.56      3000
weighted avg       0.57      0.57      0.57      3000

The epoch:1,marcoF1ScoresTotal:0.631236
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2021_1_3_18_14/save_model/roberta_best_dev_f1_0.6312363321412151.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2021_1_3_18_14/save_model/classifier_best_dev_f1_0.6312363321412151.pt
best epoch is:1 with best f1 is: 0.631236
*********************************************train model**************************************
current epoch:2
[ 1000 - th batch: train loss is 0.453846 valid total precise is 0.721722 ]
[ 2000 - th batch: train loss is 0.458894 valid total precise is 0.717192 ]
[ 3000 - th batch: train loss is 0.464979 valid total precise is 0.715238 ]
[ 4000 - th batch: train loss is 0.464641 valid total precise is 0.715068 ]
[ 5000 - th batch: train loss is 0.463337 valid total precise is 0.716832 ]
[ 6000 - th batch: train loss is 0.461664 valid total precise is 0.717175 ]
[ 7000 - th batch: train loss is 0.461727 valid total precise is 0.717118 ]
[ 8000 - th batch: train loss is 0.461432 valid total precise is 0.717465 ]
[ 9000 - th batch: train loss is 0.459452 valid total precise is 0.718265 ]
[ 10000 - th batch: train loss is 0.459112 valid total precise is 0.718739 ]
[ 11000 - th batch: train loss is 0.459767 valid total precise is 0.719257 ]
[ 12000 - th batch: train loss is 0.460480 valid total precise is 0.719273 ]
[ 13000 - th batch: train loss is 0.460622 valid total precise is 0.719867 ]
[ 14000 - th batch: train loss is 0.460913 valid total precise is 0.720075 ]
[ 15000 - th batch: train loss is 0.461294 valid total precise is 0.720581 ]
The epoch:2,marcoF1ScoresOcemotion:0.604256
********************************confusion_matrix********************************
[[1812  327    9  260   46 1291   38]
 [ 460 1936    6  379   63 1148   39]
 [  21   32  233   32   22  204    4]
 [ 138  199    8 6603  456  761   35]
 [  51   46    6  774 2199  643   15]
 [ 590  409   23  552  296 9674   24]
 [  90   73    2  131   43  228  263]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.57      0.48      0.52      3783
           1       0.64      0.48      0.55      4031
           2       0.81      0.43      0.56       548
           3       0.76      0.81      0.78      8200
           4       0.70      0.59      0.64      3734
           5       0.69      0.84      0.76     11568
           6       0.63      0.32      0.42       830

    accuracy                           0.69     32694
   macro avg       0.69      0.56      0.60     32694
weighted avg       0.69      0.69      0.69     32694

The epoch:2,marcoF1ScoresOcnli:0.829111
********************************confusion_matrix********************************
[[14031  2165   519]
 [ 2056 13797  1431]
 [  712  1780 13896]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.84      0.84      0.84     16715
           1       0.78      0.80      0.79     17284
           2       0.88      0.85      0.86     16388

    accuracy                           0.83     50387
   macro avg       0.83      0.83      0.83     50387
weighted avg       0.83      0.83      0.83     50387

The epoch:2,marcoF1ScoresTnews:0.637777
********************************confusion_matrix********************************
[[ 736   78  193    2   16   10   10   36    6   11   27   53    0   69
    19]
 [  72 3058  349   29   59   16   16  231   63   75  366  103    2  119
    34]
 [ 183  336 3869  169   47   20   61  148  116   77   74  226    0   63
   217]
 [   8   83  395 3322   54    9   64  104   44   68   61  116    0   15
   194]
 [  14   64   50   29 3561  250  113  144  981   41   87  249   41  240
    17]
 [  18   31   24   12  272 1607   33   56   52    3  130   47    0   78
     3]
 [  27   54  124   32  154   33 3306   63  357   64  233  122    0   55
    51]
 [  35  290  128   60   99   59   27 2691  147   62   74   90    0   81
    40]
 [   3   57  103   27 1278   59  315  206 4055  127   46  136    3   68
   205]
 [  14  105   93   31   51    2   71   85  108 2369   73  994    0   31
   101]
 [  22  395   90   30   79   92  167   76   71   27 2186  367    0  240
    34]
 [  63   87  246   58  233   22   86  112  117  673  254 3419    0   67
    33]
 [   0    0    0    0  163    3    2    0   12    0    0    0  110    0
     0]
 [  81  125   46   11  294   75   52   90   63   25  250   89    0 2020
    10]
 [  10   57  364  120   34    4   40   66  289  117   24   37    0   28
  2681]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.57      0.58      0.58      1266
           1       0.63      0.67      0.65      4592
           2       0.64      0.69      0.66      5606
           3       0.84      0.73      0.78      4537
           4       0.56      0.61      0.58      5881
           5       0.71      0.68      0.69      2366
           6       0.76      0.71      0.73      4675
           7       0.66      0.69      0.67      3883
           8       0.63      0.61      0.62      6688
           9       0.63      0.57      0.60      4128
          10       0.56      0.56      0.56      3876
          11       0.57      0.63      0.59      5470
          12       0.71      0.38      0.49       290
          13       0.64      0.63      0.63      3231
          14       0.74      0.69      0.71      3871

    accuracy                           0.65     60360
   macro avg       0.66      0.63      0.64     60360
weighted avg       0.65      0.65      0.65     60360

The epoch:2,marcoF1ScoresTotal:0.690381
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.798267 valid total precise is 0.668001 ]
The epoch:2,marcoF1ScoresOcemotion:0.518702
********************************confusion_matrix********************************
[[112  28   1  33   9 139  10]
 [ 48 152   1  45  11 110   7]
 [  1   3  16   8   2  16   0]
 [ 16  18   1 593  54  98   1]
 [  8  10   3  78 178  72   4]
 [ 47  42   6  72  59 806   4]
 [  7   4   0  20   1  23  23]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.47      0.34      0.39       332
           1       0.59      0.41      0.48       374
           2       0.57      0.35      0.43        46
           3       0.70      0.76      0.73       781
           4       0.57      0.50      0.53       353
           5       0.64      0.78      0.70      1036
           6       0.47      0.29      0.36        78

    accuracy                           0.63      3000
   macro avg       0.57      0.49      0.52      3000
weighted avg       0.62      0.63      0.61      3000

The epoch:2 ,marcoF1ScoresOcnli:0.810821
********************************confusion_matrix********************************
[[864  94  53]
 [139 735 127]
 [ 46 106 836]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.85      0.84      1011
           1       0.79      0.73      0.76      1001
           2       0.82      0.85      0.83       988

    accuracy                           0.81      3000
   macro avg       0.81      0.81      0.81      3000
weighted avg       0.81      0.81      0.81      3000

The epoch:2,marcoF1ScoresTnews:0.558352
********************************confusion_matrix********************************
[[ 21   9   8   0   0   1   1   7   0   1   3   2   0   6   1]
 [  7 134  24   0   5   2   1  10   2   6  18   8   0   6   2]
 [  9  30 164  10   1   2   3  10  11   5   7  17   0   2   9]
 [  0   5  17 167   4   1   5   5   3   2   1   5   0   0   6]
 [  2   6   1   1 135  20   6   7  53   3   5  14   3  17   2]
 [  0   2   0   2  14  77   1   3   5   1  10   2   0   2   0]
 [  2   2   3   2   9   4 151   3  24   3  16   9   0   2   4]
 [  3  20   5   6   8   1   3 115  16   5   3   6   0   8   1]
 [  0   1   9   5  78   4  19  13 178  10   5  11   0   5  18]
 [  1   6   5   4   5   0   5   4   8 105   1  64   0   3   9]
 [  2  22  10   4   2   6   5   4   3   3  92  17   0  15   0]
 [  5  13  11   3  13   4   3   7   4  52  10 156   0   5   0]
 [  0   0   0   0   7   0   0   0   0   0   0   0   5   0   0]
 [  2   9   3   2  17   8   4   3   5   0   9   2   0  84   1]
 [  1   5  18   7   3   1   0   1  16   2   5   3   0   2 114]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.38      0.35      0.37        60
           1       0.51      0.60      0.55       225
           2       0.59      0.59      0.59       280
           3       0.78      0.76      0.77       221
           4       0.45      0.49      0.47       275
           5       0.59      0.65      0.62       119
           6       0.73      0.65      0.68       234
           7       0.60      0.57      0.59       200
           8       0.54      0.50      0.52       356
           9       0.53      0.48      0.50       220
          10       0.50      0.50      0.50       185
          11       0.49      0.55      0.52       286
          12       0.62      0.42      0.50        12
          13       0.54      0.56      0.55       149
          14       0.68      0.64      0.66       178

    accuracy                           0.57      3000
   macro avg       0.57      0.55      0.56      3000
weighted avg       0.57      0.57      0.57      3000

The epoch:2,marcoF1ScoresTotal:0.629292
best epoch is:1 with best f1 is: 0.631236
*********************************************train model**************************************
current epoch:3
[ 1000 - th batch: train loss is 0.477993 valid total precise is 0.709265 ]
