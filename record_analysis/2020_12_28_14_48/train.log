Parameters:
root    :    /share/home/crazicoco/competition/CPFC
vocabIdName    :    vocab.txt
tokenizeModel    :    hfl/chinese-roberta-wwm-ext-large
pretrainModel    :    hfl/chinese-roberta-wwm-ext-large
saveModelAddress    :    saveModelBin/
processedDataDir    :    preprocessed_data
saveLabelIdName    :    label.pt
saveTrainIdName    :    train.pt
saveValidIdName    :    valid.pt
saveTestIdName    :    test.pt
batch_size    :    9
epoch_size    :    20
loss_calculate    :    cross-entroy
lr    :    2e-05
device    :    0
max_len    :    512
lossCalculateWay    :    general
accumulate    :    True
ifparallel    :    True
debug    :    False
logfileName    :    public
a_step    :    16
description    :    use the focal loss with decay learning rate
use_bert_layer    :    -1
record_addr    :    /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48
*********************************************train model**************************************
current epoch:0
[ 1000 - th batch: valid loss is 3.877863 valid total precise is 0.388277 ]
[ 2000 - th batch: valid loss is 2.942836 valid total precise is 0.438886 ]
[ 3000 - th batch: valid loss is 2.493739 valid total precise is 0.467637 ]
[ 4000 - th batch: valid loss is 2.200073 valid total precise is 0.494235 ]
[ 5000 - th batch: valid loss is 1.984259 valid total precise is 0.514303 ]
[ 6000 - th batch: valid loss is 1.823858 valid total precise is 0.529273 ]
[ 7000 - th batch: valid loss is 1.697039 valid total precise is 0.541284 ]
[ 8000 - th batch: valid loss is 1.597890 valid total precise is 0.550347 ]
[ 9000 - th batch: valid loss is 1.513358 valid total precise is 0.559074 ]
[ 10000 - th batch: valid loss is 1.441545 valid total precise is 0.565679 ]
[ 11000 - th batch: valid loss is 1.381529 valid total precise is 0.573274 ]
[ 12000 - th batch: valid loss is 1.330689 valid total precise is 0.578048 ]
[ 13000 - th batch: valid loss is 1.284417 valid total precise is 0.583455 ]
[ 14000 - th batch: valid loss is 1.244316 valid total precise is 0.587677 ]
[ 15000 - th batch: valid loss is 1.209028 valid total precise is 0.591447 ]
The epoch:0,marcoF1ScoresOcemotion:0.446774
********************************confusion_matrix********************************
[[1180  392    4  396   81 1708   22]
 [ 447 1374    8  541   90 1545   26]
 [  21   30  148   50   24  273    2]
 [ 197  278   13 5705  568 1418   21]
 [  70   78    8 1052 1495 1023    8]
 [ 686  539   34 1069  420 8800   20]
 [  98   78    1  219   43  296   95]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.44      0.31      0.36      3783
           1       0.50      0.34      0.40      4031
           2       0.69      0.27      0.39       548
           3       0.63      0.70      0.66      8200
           4       0.55      0.40      0.46      3734
           5       0.58      0.76      0.66     11568
           6       0.49      0.11      0.19       830

    accuracy                           0.57     32694
   macro avg       0.55      0.41      0.45     32694
weighted avg       0.56      0.57      0.56     32694

The epoch:0,marcoF1ScoresOcnli:0.663971
********************************confusion_matrix********************************
[[11231  3660  1824]
 [ 3752 10588  2944]
 [ 1852  2940 11596]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.67      0.67      0.67     16715
           1       0.62      0.61      0.61     17284
           2       0.71      0.71      0.71     16388

    accuracy                           0.66     50387
   macro avg       0.66      0.66      0.66     50387
weighted avg       0.66      0.66      0.66     50387

The epoch:0,marcoF1ScoresTnews:0.525730
********************************confusion_matrix********************************
[[ 483  130  303    7   23   16   23   48   18   12   22   70    0   95
    16]
 [  42 2662  458   40   97   20   22  280  108   85  437  138    0  151
    52]
 [ 155  452 3420  267   75   20   75  163  189   94  103  219    0   81
   293]
 [  11  107  441 3090   70   15   90  107   67   55   75  138    0   28
   243]
 [  18   90   88   47 2810  273  163  177 1277   55  133  313   57  328
    52]
 [  20   50   39   10  348 1371   67   68   90    6  149   47    0   93
     8]
 [  29   69  173   56  199   41 3021   73  403   86  245  149    0   79
    52]
 [  44  343  213   97  151   61   46 2270  221   65   92  141    0   94
    45]
 [  12   76  170   43 1395   78  439  240 3479  136   62  193    5   83
   277]
 [  11  154  155   53   65    7   96   93  144 1975   88 1094    0   44
   149]
 [  20  497  140   39  135  114  209   88  102   57 1735  444    0  256
    40]
 [  76  141  328   91  290   37  124  127  212  916  304 2691    0   89
    44]
 [   0    1    0    1  230    1    0    0   11    0    0    2   44    0
     0]
 [  81  202   83   10  339  110   63  114  110   32  297  119    0 1651
    20]
 [   9   88  428  222   57    7   54   62  346  134   46   50    0   35
  2333]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.48      0.38      0.42      1266
           1       0.53      0.58      0.55      4592
           2       0.53      0.61      0.57      5606
           3       0.76      0.68      0.72      4537
           4       0.45      0.48      0.46      5881
           5       0.63      0.58      0.60      2366
           6       0.67      0.65      0.66      4675
           7       0.58      0.58      0.58      3883
           8       0.51      0.52      0.52      6688
           9       0.53      0.48      0.50      4128
          10       0.46      0.45      0.45      3876
          11       0.46      0.49      0.48      5470
          12       0.42      0.15      0.22       290
          13       0.53      0.51      0.52      3231
          14       0.64      0.60      0.62      3871

    accuracy                           0.55     60360
   macro avg       0.55      0.52      0.53     60360
weighted avg       0.55      0.55      0.55     60360

The epoch:0,marcoF1ScoresTotal:0.545492
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.543235 valid total precise is 0.657102 ]
The epoch:0,marcoF1ScoresOcemotion:0.506935
********************************confusion_matrix********************************
[[137  13   1  35   6 136   4]
 [ 66 121   1  56  13 115   2]
 [  2   2  15   7   6  14   0]
 [ 20  12   1 590  60  98   0]
 [  5   8   2  88 180  69   1]
 [ 70  23   5  78  60 798   2]
 [ 13   4   0  25   1  18  17]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.44      0.41      0.42       332
           1       0.66      0.32      0.43       374
           2       0.60      0.33      0.42        46
           3       0.67      0.76      0.71       781
           4       0.55      0.51      0.53       353
           5       0.64      0.77      0.70      1036
           6       0.65      0.22      0.33        78

    accuracy                           0.62      3000
   macro avg       0.60      0.47      0.51      3000
weighted avg       0.62      0.62      0.60      3000

The epoch:0 ,marcoF1ScoresOcnli:0.796364
********************************confusion_matrix********************************
[[865 114  32]
 [132 765 104]
 [ 69 160 759]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.81      0.86      0.83      1011
           1       0.74      0.76      0.75      1001
           2       0.85      0.77      0.81       988

    accuracy                           0.80      3000
   macro avg       0.80      0.80      0.80      3000
weighted avg       0.80      0.80      0.80      3000

The epoch:0,marcoF1ScoresTnews:0.544274
********************************confusion_matrix********************************
[[ 22  11   9   0   1   1   2   7   0   1   0   2   0   4   0]
 [  6 154  20   1   4   2   1   8   3   2  10   8   0   6   0]
 [ 10  37 153  15   2   2   3  13  10   5  10  11   0   1   8]
 [  0   9  16 165   2   1   7   5   5   1   1   4   0   0   5]
 [  0   5   5   2 109  19   6   8  78   3   6   9   6  17   2]
 [  0   5   0   2  12  75   2   5   5   1   9   2   0   1   0]
 [  2   5   4   3   5   3 150   5  31   0  14   8   0   1   3]
 [  0  26   7   5   6   1   1 124  17   3   3   3   0   2   2]
 [  0   5   7   4  50   4  16  16 207   8   4  10   1   3  21]
 [  1  11   4   6   3   0   4   6  10 140   1  24   0   2   8]
 [  2  33  11   6   1   5   5   6   5   2  83  14   0  12   0]
 [  1  12  14   7  11   4   3  13  16  88  15  97   0   4   1]
 [  0   0   0   0   6   0   0   0   1   0   0   0   5   0   0]
 [  2   9   4   2  18   4   3   8   7   0  11   3   0  78   0]
 [  1   5  19  13   0   1   0   3  24   3   2   2   0   1 104]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.47      0.37      0.41        60
           1       0.47      0.68      0.56       225
           2       0.56      0.55      0.55       280
           3       0.71      0.75      0.73       221
           4       0.47      0.40      0.43       275
           5       0.61      0.63      0.62       119
           6       0.74      0.64      0.69       234
           7       0.55      0.62      0.58       200
           8       0.49      0.58      0.53       356
           9       0.54      0.64      0.59       220
          10       0.49      0.45      0.47       185
          11       0.49      0.34      0.40       286
          12       0.42      0.42      0.42        12
          13       0.59      0.52      0.56       149
          14       0.68      0.58      0.63       178

    accuracy                           0.56      3000
   macro avg       0.55      0.54      0.54      3000
weighted avg       0.56      0.56      0.55      3000

The epoch:0,marcoF1ScoresTotal:0.615858
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/roberta_best_dev_f1_0.6158577995048956.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/classifier_best_dev_f1_0.6158577995048956.pt
best epoch is:0 with best f1 is: 0.615858
*********************************************train model**************************************
current epoch:1
[ 1000 - th batch: valid loss is 0.373417 valid total precise is 0.693694 ]
[ 2000 - th batch: valid loss is 0.358041 valid total precise is 0.699294 ]
[ 3000 - th batch: valid loss is 0.352768 valid total precise is 0.698122 ]
[ 4000 - th batch: valid loss is 0.346376 valid total precise is 0.698619 ]
[ 5000 - th batch: valid loss is 0.343943 valid total precise is 0.697495 ]
[ 6000 - th batch: valid loss is 0.342304 valid total precise is 0.698820 ]
[ 7000 - th batch: valid loss is 0.341242 valid total precise is 0.698957 ]
[ 8000 - th batch: valid loss is 0.341327 valid total precise is 0.698504 ]
[ 9000 - th batch: valid loss is 0.339215 valid total precise is 0.699362 ]
[ 10000 - th batch: valid loss is 0.338084 valid total precise is 0.699748 ]
[ 11000 - th batch: valid loss is 0.337190 valid total precise is 0.700902 ]
[ 12000 - th batch: valid loss is 0.336050 valid total precise is 0.701901 ]
[ 13000 - th batch: valid loss is 0.335567 valid total precise is 0.702781 ]
[ 14000 - th batch: valid loss is 0.334164 valid total precise is 0.704130 ]
[ 15000 - th batch: valid loss is 0.333575 valid total precise is 0.704299 ]
The epoch:1,marcoF1ScoresOcemotion:0.580792
********************************confusion_matrix********************************
[[1761  344   12  281   47 1309   29]
 [ 468 1862   11  393   74 1191   32]
 [  27   16  240   32   26  203    4]
 [ 141  252   10 6382  552  838   25]
 [  54   52    5  802 2145  661   15]
 [ 652  468   47  640  388 9346   27]
 [ 105   78    5  158   37  218  229]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.55      0.47      0.50      3783
           1       0.61      0.46      0.52      4031
           2       0.73      0.44      0.55       548
           3       0.73      0.78      0.76      8200
           4       0.66      0.57      0.61      3734
           5       0.68      0.81      0.74     11568
           6       0.63      0.28      0.38       830

    accuracy                           0.67     32694
   macro avg       0.66      0.54      0.58     32694
weighted avg       0.67      0.67      0.66     32694

The epoch:1,marcoF1ScoresOcnli:0.814856
********************************confusion_matrix********************************
[[13904  2162   649]
 [ 2260 13209  1815]
 [  808  1665 13915]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.83      0.83     16715
           1       0.78      0.76      0.77     17284
           2       0.85      0.85      0.85     16388

    accuracy                           0.81     50387
   macro avg       0.81      0.82      0.81     50387
weighted avg       0.81      0.81      0.81     50387

The epoch:1,marcoF1ScoresTnews:0.624374
********************************confusion_matrix********************************
[[ 703   89  197    4   15   11   12   45    8    9   16   65    0   75
    17]
 [  65 3140  328   40   55   13   24  230   57   71  330  100    2  100
    37]
 [ 184  355 3760  219   44   24   63  158  116   75   79  219    0   59
   251]
 [   4   92  346 3428   53   11   58   93   30   60   58   98    0    9
   197]
 [  12   63   47   41 3210  269  130  132 1202   37   89  272   90  265
    22]
 [  17   38   18   11  269 1606   37   58   45    3  149   34    0   80
     1]
 [  28   50  121   51  155   36 3307   53  369   64  219  127    0   57
    38]
 [  20  347  138   69  109   53   40 2595  179   57   75   94    0   74
    33]
 [   4   47  112   28 1257   74  348  215 3947  136   38  144   11   70
   257]
 [  13  105   97   38   45    3   81   88  115 2428   64  895    0   38
   118]
 [  22  428   94   34   95   77  196   86   74   39 2111  327    0  260
    33]
 [  57   92  231   82  248   32   84  118  142  893  299 3105    0   56
    31]
 [   0    0    0    0  154    4    1    0    8    0    0    0  123    0
     0]
 [  85  147   44   10  331   77   49  120   53   14  222   83    0 1988
     8]
 [  16   75  325  162   29    2   41   54  284  122   26   31    0   22
  2682]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.57      0.56      0.56      1266
           1       0.62      0.68      0.65      4592
           2       0.64      0.67      0.66      5606
           3       0.81      0.76      0.78      4537
           4       0.53      0.55      0.54      5881
           5       0.70      0.68      0.69      2366
           6       0.74      0.71      0.72      4675
           7       0.64      0.67      0.65      3883
           8       0.60      0.59      0.59      6688
           9       0.61      0.59      0.60      4128
          10       0.56      0.54      0.55      3876
          11       0.56      0.57      0.56      5470
          12       0.54      0.42      0.48       290
          13       0.63      0.62      0.62      3231
          14       0.72      0.69      0.71      3871

    accuracy                           0.63     60360
   macro avg       0.63      0.62      0.62     60360
weighted avg       0.63      0.63      0.63     60360

The epoch:1,marcoF1ScoresTotal:0.673340
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.521899 valid total precise is 0.664553 ]
The epoch:1,marcoF1ScoresOcemotion:0.520976
********************************confusion_matrix********************************
[[128  20   3  37   5 132   7]
 [ 51 149   1  47  10 109   7]
 [  1   4  16   8   4  13   0]
 [ 17  21   1 586  52 102   2]
 [  6   8   3  85 173  76   2]
 [ 65  46   6  83  49 785   2]
 [  9   8   0  19   0  18  24]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.46      0.39      0.42       332
           1       0.58      0.40      0.47       374
           2       0.53      0.35      0.42        46
           3       0.68      0.75      0.71       781
           4       0.59      0.49      0.54       353
           5       0.64      0.76      0.69      1036
           6       0.55      0.31      0.39        78

    accuracy                           0.62      3000
   macro avg       0.58      0.49      0.52      3000
weighted avg       0.61      0.62      0.61      3000

The epoch:1 ,marcoF1ScoresOcnli:0.806430
********************************confusion_matrix********************************
[[877  74  60]
 [155 691 155]
 [ 36  94 858]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.87      0.84      1011
           1       0.80      0.69      0.74      1001
           2       0.80      0.87      0.83       988

    accuracy                           0.81      3000
   macro avg       0.81      0.81      0.81      3000
weighted avg       0.81      0.81      0.81      3000

The epoch:1,marcoF1ScoresTnews:0.560713
********************************confusion_matrix********************************
[[ 26   7  10   0   1   1   2   4   0   2   1   1   0   4   1]
 [ 10 133  25   1   3   2   1   6   2   4  19  10   0   9   0]
 [ 12  25 168   7   2   2   4  10  11   5  10  14   0   2   8]
 [  0   7  23 155   4   1   6   4   4   1   1   6   0   0   9]
 [  1   3   4   0 135  17   6   4  61   2   5  10   5  20   2]
 [  0   2   0   2  16  77   2   1   5   1  10   2   0   1   0]
 [  3   3   5   3   7   3 153   2  27   2  15   8   0   1   2]
 [  3  22   6   6   8   2   3 113  19   3   2   6   0   5   2]
 [  0   3   9   3  75   2  23  10 177  10   6  14   1   4  19]
 [  2   9   6   5   4   0   4   4  11  93   1  69   0   4   8]
 [  3  17  11   3   1   8   8   5   4   2  93  16   0  14   0]
 [  3  10  16   5  10   4   3   6   9  28  15 172   0   4   1]
 [  0   0   0   0   5   0   0   0   1   0   0   0   6   0   0]
 [  2   9   2   1  16   5   2   4   6   0  13   4   0  84   1]
 [  1   3  21   8   1   1   0   1  22   2   5   3   0   0 110]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.39      0.43      0.41        60
           1       0.53      0.59      0.56       225
           2       0.55      0.60      0.57       280
           3       0.78      0.70      0.74       221
           4       0.47      0.49      0.48       275
           5       0.62      0.65      0.63       119
           6       0.71      0.65      0.68       234
           7       0.65      0.56      0.60       200
           8       0.49      0.50      0.50       356
           9       0.60      0.42      0.50       220
          10       0.47      0.50      0.49       185
          11       0.51      0.60      0.55       286
          12       0.50      0.50      0.50        12
          13       0.55      0.56      0.56       149
          14       0.67      0.62      0.65       178

    accuracy                           0.56      3000
   macro avg       0.57      0.56      0.56      3000
weighted avg       0.57      0.56      0.57      3000

The epoch:1,marcoF1ScoresTotal:0.629373
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/roberta_best_dev_f1_0.629372936129997.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/classifier_best_dev_f1_0.629372936129997.pt
best epoch is:1 with best f1 is: 0.629373
*********************************************train model**************************************
current epoch:2
[ 1000 - th batch: valid loss is 0.300469 valid total precise is 0.710711 ]
[ 2000 - th batch: valid loss is 0.300690 valid total precise is 0.712634 ]
[ 3000 - th batch: valid loss is 0.295742 valid total precise is 0.714794 ]
[ 4000 - th batch: valid loss is 0.289389 valid total precise is 0.715846 ]
[ 5000 - th batch: valid loss is 0.290466 valid total precise is 0.713209 ]
[ 6000 - th batch: valid loss is 0.291473 valid total precise is 0.712878 ]
[ 7000 - th batch: valid loss is 0.289755 valid total precise is 0.714308 ]
[ 8000 - th batch: valid loss is 0.289683 valid total precise is 0.714395 ]
[ 9000 - th batch: valid loss is 0.289210 valid total precise is 0.714240 ]
[ 10000 - th batch: valid loss is 0.288482 valid total precise is 0.714305 ]
[ 11000 - th batch: valid loss is 0.288116 valid total precise is 0.714893 ]
[ 12000 - th batch: valid loss is 0.288476 valid total precise is 0.716032 ]
[ 13000 - th batch: valid loss is 0.288356 valid total precise is 0.716773 ]
[ 14000 - th batch: valid loss is 0.287460 valid total precise is 0.717615 ]
[ 15000 - th batch: valid loss is 0.287322 valid total precise is 0.718040 ]
The epoch:2,marcoF1ScoresOcemotion:0.604272
********************************confusion_matrix********************************
[[1868  348   14  252   50 1218   33]
 [ 470 1973   12  385   64 1093   34]
 [  25   22  256   30   25  189    1]
 [ 135  237   10 6502  491  793   32]
 [  53   55    3  809 2156  643   15]
 [ 649  481   44  587  329 9454   24]
 [  96   68    5  137   36  219  269]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.57      0.49      0.53      3783
           1       0.62      0.49      0.55      4031
           2       0.74      0.47      0.57       548
           3       0.75      0.79      0.77      8200
           4       0.68      0.58      0.63      3734
           5       0.69      0.82      0.75     11568
           6       0.66      0.32      0.43       830

    accuracy                           0.69     32694
   macro avg       0.67      0.57      0.60     32694
weighted avg       0.68      0.69      0.68     32694

The epoch:2,marcoF1ScoresOcnli:0.823370
********************************confusion_matrix********************************
[[14062  2018   635]
 [ 2198 13263  1823]
 [  717  1530 14141]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.83      0.84      0.83     16715
           1       0.79      0.77      0.78     17284
           2       0.85      0.86      0.86     16388

    accuracy                           0.82     50387
   macro avg       0.82      0.82      0.82     50387
weighted avg       0.82      0.82      0.82     50387

The epoch:2,marcoF1ScoresTnews:0.639631
********************************confusion_matrix********************************
[[ 758   81  187    3   12    7   17   29    7    8   19   56    0   64
    18]
 [  63 3137  339   22   54   18   21  196   62   75  388   85    0   97
    35]
 [ 179  334 3865  205   37   22   59  130  120   76   78  209    0   57
   235]
 [   9   83  335 3471   42    9   56   91   34   66   57   93    0    9
   182]
 [  15   55   42   30 3368  263  108  125 1144   31   76  233   95  270
    26]
 [  18   34   16    9  271 1628   35   57   48    2  124   35    0   87
     2]
 [  30   51  116   38  136   37 3385   55  341   71  212  121    0   47
    35]
 [  34  345  136   63  107   53   40 2603  173   63   74   97    0   65
    30]
 [   4   47   97   25 1217   65  350  190 4071  114   42  143    6   60
   257]
 [   9   97  102   37   52    4   62   61  123 2329   68 1031    0   29
   124]
 [  22  422   97   25   79   87  208   76   73   36 2200  295    0  230
    26]
 [  57   95  217   68  264   23   78  101  136  687  319 3347    0   48
    30]
 [   0    0    0    0  143    4    2    0   12    0    0    0  129    0
     0]
 [  97  128   39    9  300   73   49  102   57   21  256   77    0 2012
    11]
 [  14   61  328  142   30    2   42   44  280  108   25   35    0   22
  2738]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.58      0.60      0.59      1266
           1       0.63      0.68      0.66      4592
           2       0.65      0.69      0.67      5606
           3       0.84      0.77      0.80      4537
           4       0.55      0.57      0.56      5881
           5       0.71      0.69      0.70      2366
           6       0.75      0.72      0.74      4675
           7       0.67      0.67      0.67      3883
           8       0.61      0.61      0.61      6688
           9       0.63      0.56      0.60      4128
          10       0.56      0.57      0.56      3876
          11       0.57      0.61      0.59      5470
          12       0.56      0.44      0.50       290
          13       0.65      0.62      0.64      3231
          14       0.73      0.71      0.72      3871

    accuracy                           0.65     60360
   macro avg       0.65      0.63      0.64     60360
weighted avg       0.65      0.65      0.65     60360

The epoch:2,marcoF1ScoresTotal:0.689091
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.510001 valid total precise is 0.665110 ]
The epoch:2,marcoF1ScoresOcemotion:0.525127
********************************confusion_matrix********************************
[[120  21   3  37   5 139   7]
 [ 44 153   1  48  10 112   6]
 [  1   4  17   8   3  13   0]
 [ 17  22   1 584  53 102   2]
 [  5   8   3  83 174  78   2]
 [ 52  51   6  78  49 798   2]
 [  8   8   0  19   0  19  24]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.49      0.36      0.41       332
           1       0.57      0.41      0.48       374
           2       0.55      0.37      0.44        46
           3       0.68      0.75      0.71       781
           4       0.59      0.49      0.54       353
           5       0.63      0.77      0.69      1036
           6       0.56      0.31      0.40        78

    accuracy                           0.62      3000
   macro avg       0.58      0.49      0.53      3000
weighted avg       0.61      0.62      0.61      3000

The epoch:2 ,marcoF1ScoresOcnli:0.808711
********************************confusion_matrix********************************
[[877  84  50]
 [154 709 138]
 [ 45  98 845]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.87      0.84      1011
           1       0.80      0.71      0.75      1001
           2       0.82      0.86      0.84       988

    accuracy                           0.81      3000
   macro avg       0.81      0.81      0.81      3000
weighted avg       0.81      0.81      0.81      3000

The epoch:2,marcoF1ScoresTnews:0.555178
********************************confusion_matrix********************************
[[ 25   9   8   0   1   1   2   4   0   2   1   2   0   4   1]
 [  9 135  22   1   3   2   1   6   2   5  19  10   0   9   1]
 [ 12  28 157  11   2   2   4  11  10   6  10  14   0   2  11]
 [  0   6  18 162   4   1   6   4   3   2   1   5   0   0   9]
 [  1   3   4   0 145  17   6   5  50   3   5   9   4  21   2]
 [  0   2   0   2  16  76   2   3   4   1   9   2   0   2   0]
 [  3   3   5   3   7   3 157   3  22   2  14   8   0   2   2]
 [  2  22   6   6   8   2   3 115  17   3   2   6   0   6   2]
 [  0   1   7   5  85   2  27  12 161  11   6  13   1   5  20]
 [  1  10   5   5   4   0   4   4  11 111   1  52   0   4   8]
 [  3  17  11   3   1   8   8   5   4   2  91  17   0  15   0]
 [  3  12  13   6  12   4   3   7   8  46  14 152   0   5   1]
 [  0   0   0   0   6   0   0   0   1   0   0   0   5   0   0]
 [  2   9   2   1  17   5   3   4   3   0  12   3   0  87   1]
 [  1   3  21  11   2   1   0   1  21   2   5   3   0   0 107]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.40      0.42      0.41        60
           1       0.52      0.60      0.56       225
           2       0.56      0.56      0.56       280
           3       0.75      0.73      0.74       221
           4       0.46      0.53      0.49       275
           5       0.61      0.64      0.63       119
           6       0.69      0.67      0.68       234
           7       0.62      0.57      0.60       200
           8       0.51      0.45      0.48       356
           9       0.57      0.50      0.53       220
          10       0.48      0.49      0.49       185
          11       0.51      0.53      0.52       286
          12       0.50      0.42      0.45        12
          13       0.54      0.58      0.56       149
          14       0.65      0.60      0.62       178

    accuracy                           0.56      3000
   macro avg       0.56      0.55      0.56      3000
weighted avg       0.56      0.56      0.56      3000

The epoch:2,marcoF1ScoresTotal:0.629672
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/roberta_best_dev_f1_0.6296719255709303.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/classifier_best_dev_f1_0.6296719255709303.pt
best epoch is:2 with best f1 is: 0.629672
*********************************************train model**************************************
current epoch:3
[ 1000 - th batch: valid loss is 0.287279 valid total precise is 0.715604 ]
[ 2000 - th batch: valid loss is 0.285290 valid total precise is 0.715135 ]
[ 3000 - th batch: valid loss is 0.284867 valid total precise is 0.715016 ]
[ 4000 - th batch: valid loss is 0.282816 valid total precise is 0.717152 ]
[ 5000 - th batch: valid loss is 0.283419 valid total precise is 0.716143 ]
[ 6000 - th batch: valid loss is 0.282729 valid total precise is 0.716416 ]
[ 7000 - th batch: valid loss is 0.282679 valid total precise is 0.717007 ]
[ 8000 - th batch: valid loss is 0.282582 valid total precise is 0.716215 ]
[ 9000 - th batch: valid loss is 0.282205 valid total precise is 0.715721 ]
[ 10000 - th batch: valid loss is 0.282412 valid total precise is 0.716083 ]
[ 11000 - th batch: valid loss is 0.281639 valid total precise is 0.716782 ]
[ 12000 - th batch: valid loss is 0.281890 valid total precise is 0.717050 ]
[ 13000 - th batch: valid loss is 0.281482 valid total precise is 0.717730 ]
[ 14000 - th batch: valid loss is 0.281035 valid total precise is 0.718861 ]
[ 15000 - th batch: valid loss is 0.281063 valid total precise is 0.718929 ]
The epoch:3,marcoF1ScoresOcemotion:0.605384
********************************confusion_matrix********************************
[[1851  360   13  243   48 1233   35]
 [ 474 2022    8  370   64 1063   30]
 [  16   22  254   31   30  192    3]
 [ 132  225   11 6515  495  790   32]
 [  44   50    4  797 2187  640   12]
 [ 649  505   38  590  332 9430   24]
 [  95   69    4  139   43  218  262]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.57      0.49      0.53      3783
           1       0.62      0.50      0.56      4031
           2       0.77      0.46      0.58       548
           3       0.75      0.79      0.77      8200
           4       0.68      0.59      0.63      3734
           5       0.70      0.82      0.75     11568
           6       0.66      0.32      0.43       830

    accuracy                           0.69     32694
   macro avg       0.68      0.57      0.61     32694
weighted avg       0.68      0.69      0.68     32694

The epoch:3,marcoF1ScoresOcnli:0.825385
********************************confusion_matrix********************************
[[14102  1992   621]
 [ 2251 13360  1673]
 [  722  1566 14100]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.83      0.84      0.83     16715
           1       0.79      0.77      0.78     17284
           2       0.86      0.86      0.86     16388

    accuracy                           0.82     50387
   macro avg       0.83      0.83      0.83     50387
weighted avg       0.82      0.82      0.82     50387

The epoch:3,marcoF1ScoresTnews:0.641078
********************************confusion_matrix********************************
[[ 745   91  184    3   13   11   18   30    7   10   18   55    0   65
    16]
 [  66 3158  327   31   52   15   16  220   47   75  383   74    1   96
    31]
 [ 171  344 3844  217   38   19   55  137  123   77   77  217    0   57
   230]
 [   7   82  322 3513   32    8   56   92   33   62   54   89    0   11
   176]
 [  13   51   39   28 3524  261  118  129 1016   30   79  229   82  262
    20]
 [  16   40   17    7  272 1636   33   54   49    0  127   29    0   86
     0]
 [  29   56  122   43  149   32 3403   43  300   75  208  120    0   59
    36]
 [  30  323  130   77  115   51   36 2623  150   72   71   95    0   75
    35]
 [   3   45  103   23 1346   66  371  197 3911  120   30  130    5   71
   267]
 [  16  101   94   43   48    1   74   69  108 2436   71  918    0   34
   115]
 [  21  395   93   30   89   93  196   76   68   40 2199  318    0  238
    20]
 [  55   92  241   85  262   28   84  110  123  815  304 3179    0   64
    28]
 [   0    0    0    0  152    3    2    0    7    0    0    0  126    0
     0]
 [  78  116   45   11  309   81   46  104   48   20  224   76    0 2064
     9]
 [  11   66  320  150   29    2   39   48  274  103   30   34    0   22
  2743]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.59      0.59      0.59      1266
           1       0.64      0.69      0.66      4592
           2       0.65      0.69      0.67      5606
           3       0.82      0.77      0.80      4537
           4       0.55      0.60      0.57      5881
           5       0.71      0.69      0.70      2366
           6       0.75      0.73      0.74      4675
           7       0.67      0.68      0.67      3883
           8       0.62      0.58      0.60      6688
           9       0.62      0.59      0.60      4128
          10       0.57      0.57      0.57      3876
          11       0.57      0.58      0.58      5470
          12       0.59      0.43      0.50       290
          13       0.64      0.64      0.64      3231
          14       0.74      0.71      0.72      3871

    accuracy                           0.65     60360
   macro avg       0.65      0.64      0.64     60360
weighted avg       0.65      0.65      0.65     60360

The epoch:3,marcoF1ScoresTotal:0.690616
*********************************************start valid model**************************************
[ 1000 - th batch: valid loss is 0.521203 valid total precise is 0.665777 ]
The epoch:3,marcoF1ScoresOcemotion:0.524611
********************************confusion_matrix********************************
[[119  20   3  37   5 141   7]
 [ 44 153   1  47  10 113   6]
 [  1   4  17   8   3  13   0]
 [ 17  22   1 582  54 103   2]
 [  5   8   3  83 174  78   2]
 [ 50  51   6  79  49 799   2]
 [  8   8   0  19   0  19  24]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.49      0.36      0.41       332
           1       0.58      0.41      0.48       374
           2       0.55      0.37      0.44        46
           3       0.68      0.75      0.71       781
           4       0.59      0.49      0.54       353
           5       0.63      0.77      0.69      1036
           6       0.56      0.31      0.40        78

    accuracy                           0.62      3000
   macro avg       0.58      0.49      0.52      3000
weighted avg       0.61      0.62      0.61      3000

The epoch:3 ,marcoF1ScoresOcnli:0.812201
********************************confusion_matrix********************************
[[888  74  49]
 [151 709 141]
 [ 41 102 845]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.82      0.88      0.85      1011
           1       0.80      0.71      0.75      1001
           2       0.82      0.86      0.84       988

    accuracy                           0.81      3000
   macro avg       0.81      0.81      0.81      3000
weighted avg       0.81      0.81      0.81      3000

The epoch:3,marcoF1ScoresTnews:0.554828
********************************confusion_matrix********************************
[[ 25   9   8   0   1   1   2   4   0   2   1   2   0   4   1]
 [  9 135  22   1   3   2   1   6   2   5  19  10   0   9   1]
 [ 12  28 157  11   2   2   4  11  10   6  10  14   0   2  11]
 [  0   6  18 161   4   1   6   4   3   2   1   5   0   0  10]
 [  1   3   4   0 140  17   6   5  55   2   5  10   4  21   2]
 [  0   2   0   2  16  76   2   3   4   1   9   2   0   2   0]
 [  3   3   5   3   7   3 156   3  23   2  14   8   0   2   2]
 [  2  22   6   6   8   2   3 115  17   3   2   6   0   6   2]
 [  0   1   7   5  80   3  26  12 165  11   6  14   1   5  20]
 [  1  10   5   5   4   0   4   4  11 109   1  54   0   4   8]
 [  3  17  11   3   1   8   8   5   4   2  91  17   0  15   0]
 [  3  11  14   6  11   4   3   7   9  45  12 155   0   5   1]
 [  0   0   0   0   6   0   0   0   1   0   0   0   5   0   0]
 [  2   9   2   1  16   5   3   4   4   0  12   3   0  87   1]
 [  1   3  20  11   2   1   0   1  21   2   5   3   0   0 108]]
*****************************classification_report******************************
              precision    recall  f1-score   support

           0       0.40      0.42      0.41        60
           1       0.52      0.60      0.56       225
           2       0.56      0.56      0.56       280
           3       0.75      0.73      0.74       221
           4       0.47      0.51      0.49       275
           5       0.61      0.64      0.62       119
           6       0.70      0.67      0.68       234
           7       0.62      0.57      0.60       200
           8       0.50      0.46      0.48       356
           9       0.57      0.50      0.53       220
          10       0.48      0.49      0.49       185
          11       0.51      0.54      0.53       286
          12       0.50      0.42      0.45        12
          13       0.54      0.58      0.56       149
          14       0.65      0.61      0.63       178

    accuracy                           0.56      3000
   macro avg       0.56      0.55      0.55      3000
weighted avg       0.56      0.56      0.56      3000

The epoch:3,marcoF1ScoresTotal:0.630547
save the pretrainModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/roberta_best_dev_f1_0.6305469626981502.pt and the classifierModel in /share/home/crazicoco/competition/CPFC/record_analysis/2020_12_28_14_48/save_model/classifier_best_dev_f1_0.6305469626981502.pt
best epoch is:3 with best f1 is: 0.630547
*********************************************train model**************************************
current epoch:4
[ 1000 - th batch: valid loss is 0.280439 valid total precise is 0.707596 ]
[ 2000 - th batch: valid loss is 0.278264 valid total precise is 0.715247 ]
[ 3000 - th batch: valid loss is 0.276981 valid total precise is 0.719647 ]
[ 4000 - th batch: valid loss is 0.275020 valid total precise is 0.719513 ]
[ 5000 - th batch: valid loss is 0.273925 valid total precise is 0.720611 ]
[ 6000 - th batch: valid loss is 0.274391 valid total precise is 0.720991 ]
[ 7000 - th batch: valid loss is 0.274660 valid total precise is 0.719801 ]
[ 8000 - th batch: valid loss is 0.274810 valid total precise is 0.719118 ]
[ 9000 - th batch: valid loss is 0.274899 valid total precise is 0.717771 ]
